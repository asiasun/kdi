#----------------------------------------------------------------------------
# Come back to this...
#----------------------------------------------------------------------------
    // Questions:
    //   - 

    // How are compactions / splits / joins triggered?
    //   - could register for events from FragmentGraph



    // --- Fragment operations from Tablet.cc ---

    // load fragments from config: Tablet()

    // untrack fragments: ~Tablet()

    // -- Move to Scanner
    // build a merged scan: getMergedScan(pred)


    // -- Move to ConfigWriter
    // build a list of fragment URIs for config save: getFragmentUris()


    // make clone forwarding chain: splitTablet() (should not be
    //    necessary with fragment graph)

    // clone all fragments and add references: Tablet(Tablet const &)
    

    // add a new fragment to merge list: addFragment()

    // add up disk space used by tablet: getDiskSize()

    // find a median split row: chooseSplitRow()

    // --- Move to compactor
    // choose a compaction sequence, make a new frag, replace: doCompaction()
    // replace old fragments, dropping references: replaceFragments()
    // count number of fragments: getCompactionPriority()



    // Maybe need FragmentLoader:
    //   virtual auto_ptr<Fragment> loadFragment(string const & uri) = 0;
    //
    // This interface means that caching will have to be done above
    // the loader since it passes ownership.

    // How should config changes be handled?
    // Serialized tablet config contains:
    //   - Tablet range information
    //   - Per-tablet fragment lists  (serialized form of FragmentGraph)
    //   - Server designation  (broken, should be separate column)
    //
    // Fragment graph:
    //
    //   Two classes of nodes:  Tablet (T) and Fragment (F)
    //
    //   Serialized, we have a list of Tablets, each with an ordered
    //   list of Fragments:
    //
    //      T_i : [ F_x, F_y, F_z ]
    //      T_j : [ F_w, F_z ]
    //
    //
    //  Graph form:
    //
    //      T_i  -->  F_x  -->  F_y  --> F_z
    //      T_j  -->  F_w  ---------------^
    //
    //  Alternatively: (allows Fragments to determine their Tablet set
    //  quickly, could also be done with an index)
    //
    //      T_i -------v---------v--------v
    //                F_x  -->  F_y  --> F_z
    //                F_w  ---------------^
    //      T_j -------^------------------^

    // We seem to have a RangeMap, which owns all the Tablets and maps
    // row ranges to the appropriate one, and we have a FragmentGraph,
    // which owns all the Fragments tracks the relationships between
    // them.
    
    // Many operations need only lock the FragmentGraph:
    //   - compaction
    //   - adding a log
    //   - serializing/replacing a log

    // Some need to lock both the RangeMap and the FragmentGraph:
    //   - splitting
    //   - writing configs
    
    // Where does scanning fit in?

    // Starting a scan requires locking the RangeMap and FragmentGraph:
    //   - get the approprate tablet
    //   - start a merge on all fragments
    
    // Scanners operate one tablet at time.  They need to know when
    // some tablet events happen:
    //   - the fragment list changes (replacement or add, often ignore add)
    //   - the tablet is involved in a split and the row range shrinks
    //   - the tablet is involved in a join and is invalidated

    // Expected TabletListeners: Scanner, ConfigWriter, TabletGc
    // Expected Providers: Tablet, FragmentGraph?, RangeMap?
    //   provider dispatch: buffer events when locks are held, release
    //   locks, dispatch events?

    class TabletEventDispatcher
    {
        typedef std::tr1::unordered_multimap<Tablet*, TabletListener*> mmap_t;

        mmap_t listeners;
        boost::thread::id dispatchId;
        
        boost::mutex mapMutex;
        boost::mutex dispatchMutex;

        vector<TabletListener*> getListeners(Tablet* tablet) const
        {
            vector<TabletListener*> r;

            typedef mmap_t::const_iterator citer;
            typedef std::pair<citer,citer> range_t;

            {
                boost::mutex::scoped_lock lock(mapMutex);
                
                range_t specific = listeners.equal_range(tablet);
                range_t general = listeners.equal_range(0);
                
                r.insert(r.end(), specific.begin(), specific.end());
                r.insert(r.end(), general.begin(), general.end());
            }

            std::sort(r.begin(), r.end());
            r.erase(std::unique(r.begin(), r.end()), r.end());
            
            return r;
        }

    protected:
        void dispatchFragmentListUpdate(
            Tablet* tablet, size_t beginPos, size_t endPos) const
        {
            vector<TabletListener*> l = getListeners(tablet);
            
            boost::mutex::scoped_lock lock(dispatchMutex);
            dispatchId = boost::this_thread::get_id();

            for(vector<TabletListener*>::const_iterator i = l.begin();
                i != l.end(); ++i)
            {
                // No throw guarantee
                (*i)->onFragmentListUpdate(tablet, beginPos, endPos);
            }

            dispatchId = boost::thread::id();
        }

        void dispatchRowRangeUpdate(
            Tablet* tablet, Interval<string> const & newRange) const
        {
            boost::mutex::scoped_lock lock(dispatchMutex);

        }
        
    public:
        /// Add a listener to notification list the given tablet.  If
        /// tablet is null, listen for events on ALL tablets.  Each
        /// call must be balanced by an equivalent call to
        /// removeTabletListener().  This may be called from a
        /// TabletListener callback, but it will not go into effect
        /// until the current dispatch is done.
        void addTabletListener(TabletListener* l, Tablet* tablet);


        /// Remove a listener from the notification list for the given
        /// tablet.  If tablet is null, remove the listener from
        /// all-tablet notification list.  The listener must have been
        /// added previously.  This may be called from a
        /// TabletListener callback, but it will not go into effect
        /// until the current dispatch is done.
        void removeTabletListener(TabletListener* l, Tablet* tablet);
    };


    class TabletListener
    {
    public:
        virtual ~TabletListener() {}

        /// This event is triggered when the tablet has replaced
        /// fragments in the range [beginPos,endPos) with a new
        /// fragment.  For an insertion without replacement, the range
        /// may be empty.
        virtual void
        onFragmentListUpdate(
            Tablet * tablet, size_t beginPos, size_t endPos) throws() {}

        /// This event is triggered when the tablet's row range has
        /// changed due to a split or join event.  If the tablet has
        /// been split, the new range will be smaller than the old
        /// range (but not empty).  If the tablet has been joined,
        /// either the new range will be larger than before, or the
        /// new range will be empty.
        virtual void
        onRowRangeUpdate(
            Tablet * tablet, Interval<string> const & newRange) throws() {}
    };



#----------------------------------------------------------------------------
# Server RPC 
#----------------------------------------------------------------------------

TabletServer:

    Table open(table)

    ... load(table, rangeSpec)
    ... drop(table, rangeSpec)


Table:

    apply(packedCells, commitMaxTxn, waitForSync, out commitTxn)
    sync(syncAtLeastTxn, out syncTxn)

    Scanner scan(pred)

    ... stat(table, pred, out cellStats)

    close()


Scanner:

    get(maxResponse, out packedCells, out scanTxn, out endOfScan)
    close()


#----------------------------------------------------------------------------
# Object Layout
#----------------------------------------------------------------------------

TabletServer:
    fragmentGraph  -- track connections 
    tableMap       -- map table name to RangeMap
    
RangeMap:
    tabletMap : lastRow --> Tablet



Server.main():

    RangeMap         rangeMap;        // maps rows and row ranges to tablets
    FragmentGraph    fragmentGraph;   // tracks connections between fragments and tablets

    MyFragmentLoader fragmentLoader;
    FragmentCache    fragmentCache;
    MyFragmentWriter fragmentWriter;
    MyLogWriter      logWriter;


    TabletServer server(
       &rangeMap, &fragmentGraph,
       

    Tablet meta


   
#----------------------------------------------------------------------------
# Actions
#----------------------------------------------------------------------------

RPC: TabletServer.open(name)

  - map name to Table, create if necessary
  - return table proxy

RPC: Table.apply(packedCells, commitMaxTxn, waitForSync, out commitTxn)

  - sanity: check packedCells for integrity, order
    - else throw error
  - gather list of affected rows
  - Atomic:
    - verify: all rows map to loaded, active tablets
      - else throw error
    - verify: latest txn on each modified row is <= commitMaxTxn
      - else throw row transaction conflict
    - throttle: if logger.queuedSz > MAX_QUEUE_SIZE:
      - unlock, block on logger.queuedSz, retry Atomic:
    - commitTxn = sequencer.nextTxn++
    - update txn for all affected rows
    - append the packedCells to log queue
      - logger.queuedSz += packedBuffer.size()
  - if waitForSync:
    -- also need to wait until tablet configs have been committed
    - defer until sequencer.committedTxn >= commitTxn
  - else:
    - return commitTxn

RPC: Table.sync(syncAtLeastTxn, out syncTxn)

  -- also need to wait until tablet configs have been committed
  - if sequencer.committedTxn >= syncAtLeastTxn:
    - return syncTxn = sequencer.committedTxn
  - else:
    - defer until sequencer.committedTxn >= syncAtLeastTxn

RPC: Table.scan(pred)

  - allocate a new scanner from LRU
  - init scanner with predicate
  - return scanner proxy

RPC: Table.close()

  - no op

RPC: Scanner.get(maxResponse, out packedCells, out scanTxn, out endOfScan)

  - 

RPC: Scanner.close()

  - release scanner from LRU

Callback: Tablet.onFragmentListUpdate()

  - 

Callback: Scanner.onFragmentListUpdate()

Callback: Scanner.onRowRangeUpdate()

Worker: Log

  - loop on queue:
    - get packed cell buffer
    - throttle: block until logger.committedBufferSz < MAX_BUFFER_SIZE
    - opt: while queue is not empty, gather more buffers
    - if there is no log file, make a new one
    - organize buffer(s) by table
      - for each table, if there is no log fragment, create one
      - make compacted buffer for each table
      - track all modified rows in each table
    - prepare log entry with checksum
    - write log entry to log file
    - sync log file
    - Atomic:
      - sequencer.committedTxn += (number of committed buffers)
      - for each table in commit:
        - append compacted buffer to log fragment
          - logger.committedBufferSz += buffer.size()
        - for each tablet overlapping affected rows
          - if tablet doesn't end with table log fragment
            - append table log fragment to tablet
              - call onFragmentListUpdate (w/ seq.committedTxn)
    - signal sleepers on committedTxn
    - release unused buffers
      - logger.queuedBufferSz -= sum(b.size() for b in unused buffers)
    - Atomic:
      - if active size > MAX_ACTIVE_SIZE
        - queue all log fragments for serialization
        - reset active log:
          - close log file
          - forget all fragments

Worker: Compact

  - compact.hasWork = false
  - until shutdown:
    - Atomic:
      - while not compact.hasWork:
        - wait on compact.wakeup
      - find compaction set
      - if compaction set is too puny:
        - compact.hasWork = false
        - continue
    - compact the compaction set
      - open all fragments in a merge
      - write multiple compacted outputs
      - each output is complete
        - Atomic:
          - replace old fragments with new on active tablets
            - call TabletListener.onFragmentListUpdate

Worker: Serialize

  - loop on queue:
    - get log fragment to serialize
    - compact fragment to new fragment file
    - finalize and sync new fragment
    - load new fragment
    - Atomic:
      - replace log fragment with new fragment
      - logger.committedBufferSz -= logFragment.size()
    - release log fragment
    - signal sleepers on logger.committedBufferSz

Worker: WriteConfigs

Worker: FileGc

Worker: CommitWait


#----------------------------------------------------------------------------
# Sequencing
#----------------------------------------------------------------------------

sequencer.nextTxn                 -- next txn to apply
sequencer.committedTxn            -- 


Commit stages:

  - Staged: txn allocated, queued for logger
  - Committed: cells logged

Sequencer:

  - nextTxn = 0
  - committedTxn = 0
  
  - futures = {}
  
  - atomic allocateTxn(): return nextTxn++
  - atomic commitTxn(): ++committedTxn; dispatch callbacks
  - atomic addCommitCallback(commitTargetTxn, callback)
  - atomic addFutureDeps(txn, nFutures):
    - txn (and anything after it) cannot be considered stable until
      another N future dependencies have been resolved and committed
 ** - this language is a bit strong.  maybe we can take out the (and
      anything after it) if we can guarantee that future txns will log
      the closure of their deps (possible?)
  - atomic resolveFutureDeps(txn, nFutures, futureTxn):
    - resolve N future deps on futureTxn


Cascade:  Table::open("foo")->set(X);

  - nextTxn == 1, committedTxn == 0
  - Queue commit: log cells (T=foo,P=X,txn=1,nextTxn=2)
  - Commit: log cells (T=foo,P=X)
    - create file log/1
    - create fragment F=log/1?t=foo
    - committedTxn = 1
    - add F to foo:1,foo:4,foo:5
      - Queue config (T=foo:1,txn=1)
        - add config dep: txn 1 depends on 1 config
      - Queue config (T=foo:4,txn=1)
        - add config dep: txn 1 depends on 2 configs 
      - Queue config (T=foo:5,txn=1)
        - add config dep: txn 1 depends on 3 configs 
  - Config: save (T=foo:1,txn=1)
    - Set data in META table
      - Queue commit: log cells (T=META,P=cf1,txn=2,nextTxn=3)
      - replace 1 config dep(s): txn 1 depends on txn 2 and 2 configs
  - Config: save (T=foo:4,txn=1), (T=foo:5,txn=1)  -- batch multiple
    - Set data in META table
      - Queue commit: log cells (T=META,P=cf4-5,txn=3,nextTxn=4)
      - replace 2 config dep(s): txn 1 depends on txn 3
        - (track max txn dep)
  - Commit: log cells (T=META,P=cf1)
    - reuse file log/1
    - create fragment F=log/1?t=META
    - committedTxn = 2
    - add F to META:0
      - Queue config: (T=META,txn=2)
        - add config dep: txn 2 depends on 1 config
  - Config: save (T=META:0,txn=2)
    - Queue commit: write root (T=META,P=cm0,txn=4,nextTxn=5)
      - replace 1 config dep(s): txn 2 depends on txn 4
  - Commit: log cells (T=META,P=cf4-5)
    - reuse file log/1
    - reuse fragment F=log/1?t=foo
    - committedTxn = 3
  - Commit: write root (T=META,P=cm0)
    - Write root config
    - committedTxn = 4
  

