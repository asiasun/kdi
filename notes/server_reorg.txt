#----------------------------------------------------------------------------
# Come back to this...
#----------------------------------------------------------------------------
    // Questions:
    //   - 

    // How are compactions / splits / joins triggered?
    //   - could register for events from FragmentGraph



    // --- Fragment operations from Tablet.cc ---

    // load fragments from config: Tablet()

    // untrack fragments: ~Tablet()

    // -- Move to Scanner
    // build a merged scan: getMergedScan(pred)


    // -- Move to ConfigWriter
    // build a list of fragment URIs for config save: getFragmentUris()


    // make clone forwarding chain: splitTablet() (should not be
    //    necessary with fragment graph)

    // clone all fragments and add references: Tablet(Tablet const &)
    

    // add a new fragment to merge list: addFragment()

    // add up disk space used by tablet: getDiskSize()

    // find a median split row: chooseSplitRow()

    // --- Move to compactor
    // choose a compaction sequence, make a new frag, replace: doCompaction()
    // replace old fragments, dropping references: replaceFragments()
    // count number of fragments: getCompactionPriority()



    // Maybe need FragmentLoader:
    //   virtual auto_ptr<Fragment> loadFragment(string const & uri) = 0;
    //
    // This interface means that caching will have to be done above
    // the loader since it passes ownership.

    // How should config changes be handled?
    // Serialized tablet config contains:
    //   - Tablet range information
    //   - Per-tablet fragment lists  (serialized form of FragmentGraph)
    //   - Server designation  (broken, should be separate column)
    //
    // Fragment graph:
    //
    //   Two classes of nodes:  Tablet (T) and Fragment (F)
    //
    //   Serialized, we have a list of Tablets, each with an ordered
    //   list of Fragments:
    //
    //      T_i : [ F_x, F_y, F_z ]
    //      T_j : [ F_w, F_z ]
    //
    //
    //  Graph form:
    //
    //      T_i  -->  F_x  -->  F_y  --> F_z
    //      T_j  -->  F_w  ---------------^
    //
    //  Alternatively: (allows Fragments to determine their Tablet set
    //  quickly, could also be done with an index)
    //
    //      T_i -------v---------v--------v
    //                F_x  -->  F_y  --> F_z
    //                F_w  ---------------^
    //      T_j -------^------------------^

    // We seem to have a RangeMap, which owns all the Tablets and maps
    // row ranges to the appropriate one, and we have a FragmentGraph,
    // which owns all the Fragments tracks the relationships between
    // them.
    
    // Many operations need only lock the FragmentGraph:
    //   - compaction
    //   - adding a log
    //   - serializing/replacing a log

    // Some need to lock both the RangeMap and the FragmentGraph:
    //   - splitting
    //   - writing configs
    
    // Where does scanning fit in?

    // Starting a scan requires locking the RangeMap and FragmentGraph:
    //   - get the approprate tablet
    //   - start a merge on all fragments
    
    // Scanners operate one tablet at time.  They need to know when
    // some tablet events happen:
    //   - the fragment list changes (replacement or add, often ignore add)
    //   - the tablet is involved in a split and the row range shrinks
    //   - the tablet is involved in a join and is invalidated

    // Expected TabletListeners: Scanner, ConfigWriter, TabletGc
    // Expected Providers: Tablet, FragmentGraph?, RangeMap?
    //   provider dispatch: buffer events when locks are held, release
    //   locks, dispatch events?

    class TabletEventDispatcher
    {
        typedef std::tr1::unordered_multimap<Tablet*, TabletListener*> mmap_t;

        mmap_t listeners;
        boost::thread::id dispatchId;
        
        boost::mutex mapMutex;
        boost::mutex dispatchMutex;

        vector<TabletListener*> getListeners(Tablet* tablet) const
        {
            vector<TabletListener*> r;

            typedef mmap_t::const_iterator citer;
            typedef std::pair<citer,citer> range_t;

            {
                boost::mutex::scoped_lock lock(mapMutex);
                
                range_t specific = listeners.equal_range(tablet);
                range_t general = listeners.equal_range(0);
                
                r.insert(r.end(), specific.begin(), specific.end());
                r.insert(r.end(), general.begin(), general.end());
            }

            std::sort(r.begin(), r.end());
            r.erase(std::unique(r.begin(), r.end()), r.end());
            
            return r;
        }

    protected:
        void dispatchFragmentListUpdate(
            Tablet* tablet, size_t beginPos, size_t endPos) const
        {
            vector<TabletListener*> l = getListeners(tablet);
            
            boost::mutex::scoped_lock lock(dispatchMutex);
            dispatchId = boost::this_thread::get_id();

            for(vector<TabletListener*>::const_iterator i = l.begin();
                i != l.end(); ++i)
            {
                // No throw guarantee
                (*i)->onFragmentListUpdate(tablet, beginPos, endPos);
            }

            dispatchId = boost::thread::id();
        }

        void dispatchRowRangeUpdate(
            Tablet* tablet, Interval<string> const & newRange) const
        {
            boost::mutex::scoped_lock lock(dispatchMutex);

        }
        
    public:
        /// Add a listener to notification list the given tablet.  If
        /// tablet is null, listen for events on ALL tablets.  Each
        /// call must be balanced by an equivalent call to
        /// removeTabletListener().  This may be called from a
        /// TabletListener callback, but it will not go into effect
        /// until the current dispatch is done.
        void addTabletListener(TabletListener* l, Tablet* tablet);


        /// Remove a listener from the notification list for the given
        /// tablet.  If tablet is null, remove the listener from
        /// all-tablet notification list.  The listener must have been
        /// added previously.  This may be called from a
        /// TabletListener callback, but it will not go into effect
        /// until the current dispatch is done.
        void removeTabletListener(TabletListener* l, Tablet* tablet);
    };


    class TabletListener
    {
    public:
        virtual ~TabletListener() {}

        /// This event is triggered when the tablet has replaced
        /// fragments in the range [beginPos,endPos) with a new
        /// fragment.  For an insertion without replacement, the range
        /// may be empty.
        virtual void
        onFragmentListUpdate(
            Tablet * tablet, size_t beginPos, size_t endPos) throws() {}

        /// This event is triggered when the tablet's row range has
        /// changed due to a split or join event.  If the tablet has
        /// been split, the new range will be smaller than the old
        /// range (but not empty).  If the tablet has been joined,
        /// either the new range will be larger than before, or the
        /// new range will be empty.
        virtual void
        onRowRangeUpdate(
            Tablet * tablet, Interval<string> const & newRange) throws() {}
    };



#----------------------------------------------------------------------------
# Server RPC 
#----------------------------------------------------------------------------

See src/cc/kdi/rpc/TabletServer.ice


#----------------------------------------------------------------------------
# Object Layout
#----------------------------------------------------------------------------

TabletServer:
    fragmentGraph  -- track connections 
    tableMap       -- map table name to RangeMap
    
RangeMap:
    tabletMap : lastRow --> Tablet



Server.main():

    RangeMap         rangeMap;        // maps rows and row ranges to tablets
    FragmentGraph    fragmentGraph;   // tracks connections between fragments and tablets

    MyFragmentLoader fragmentLoader;
    FragmentCache    fragmentCache;
    MyFragmentWriter fragmentWriter;
    MyLogWriter      logWriter;


    TabletServer server(
       &rangeMap, &fragmentGraph,
       

    Tablet meta


   
#----------------------------------------------------------------------------
# Actions
#----------------------------------------------------------------------------

Load Tablets
  Triggers: TabletServer API call

Unload Tablets
  Triggers: TabletServer API call

Apply Mutations
  Triggers: TabletServer API call

Split Tablet
  Triggers: A Tablet becomes too large.
    1) Event posted by the Tablet or Table as it adds new fragments.
    2) Check for splits after completing a Tablet load.
    3) Check for splits after a Table schema change.
  Requires: Tablet must be active.

Join Tablets
  Triggers: A Tablet becomes too small.
    1) Event posted by the Table as fragments get replaced and removed.
    2) Check for joins after completing a Tablet load.
    3) Check for joins after a Table schema change.
  Requires: The Table must have an adjacent Tablet to join with.  Both
    Tablets involved in the join must be active.

Compact Fragments
  Triggers: Dedicated thread loop






RPC: TabletServer.open(name)

  - map name to Table, create if necessary
  - return table proxy

RPC: Table.apply(packedCells, commitMaxTxn, waitForSync, out commitTxn)

  - sanity: check packedCells for integrity, order
    - else throw error
  - gather list of affected rows
  - Atomic:
    - verify: all rows map to loaded, active tablets
      - else throw error
    - verify: latest txn on each modified row is <= commitMaxTxn
      - else throw row transaction conflict
    - throttle: if logger.queuedSz > MAX_QUEUE_SIZE:
      - unlock, block on logger.queuedSz, retry Atomic:
    - commitTxn = sequencer.nextTxn++
    - update txn for all affected rows
    - append the packedCells to log queue
      - logger.queuedSz += packedBuffer.size()
  - if waitForSync:
    -- also need to wait until tablet configs have been committed
    - defer until sequencer.committedTxn >= commitTxn
  - else:
    - return commitTxn

RPC: Table.sync(syncAtLeastTxn, out syncTxn)

  -- also need to wait until tablet configs have been committed
  - if sequencer.committedTxn >= syncAtLeastTxn:
    - return syncTxn = sequencer.committedTxn
  - else:
    - defer until sequencer.committedTxn >= syncAtLeastTxn

RPC: Table.scan(pred)

  - allocate a new scanner from LRU
  - init scanner with predicate
  - return scanner proxy

RPC: Table.close()

  - no op

RPC: Scanner.get(maxResponse, out packedCells, out scanTxn, out endOfScan)

  - 

RPC: Scanner.close()

  - release scanner from LRU

Callback: Tablet.onFragmentListUpdate()

  - 

Callback: Scanner.onFragmentListUpdate()

Callback: Scanner.onRowRangeUpdate()

Worker: Log

  - loop on queue:
    - get packed cell buffer
    - throttle: block until logger.committedBufferSz < MAX_BUFFER_SIZE
    - opt: while queue is not empty, gather more buffers
    - if there is no log file, make a new one
    - organize buffer(s) by table
      - for each table, if there is no log fragment, create one
      - make compacted buffer for each table
      - track all modified rows in each table
    - prepare log entry with checksum
    - write log entry to log file
    - sync log file
    - Atomic:
      - sequencer.committedTxn += (number of committed buffers)
      - for each table in commit:
        - append compacted buffer to log fragment
          - logger.committedBufferSz += buffer.size()
        - for each tablet overlapping affected rows
          - if tablet doesn't end with table log fragment
            - append table log fragment to tablet
              - call onFragmentListUpdate (w/ seq.committedTxn)
    - signal sleepers on committedTxn
    - release unused buffers
      - logger.queuedBufferSz -= sum(b.size() for b in unused buffers)
    - Atomic:
      - if active size > MAX_ACTIVE_SIZE
        - queue all log fragments for serialization
        - reset active log:
          - close log file
          - forget all fragments

Worker: Compact

  - compact.hasWork = false
  - until shutdown:
    - Atomic:
      - while not compact.hasWork:
        - wait on compact.wakeup
      - find compaction set
      - if compaction set is too puny:
        - compact.hasWork = false
        - continue
    - compact the compaction set
      - open all fragments in a merge
      - write multiple compacted outputs
      - each output is complete
        - Atomic:
          - replace old fragments with new on active tablets
            - call TabletListener.onFragmentListUpdate

Worker: Serialize

  - loop on queue:
    - get log fragment to serialize
    - compact fragment to new fragment file
    - finalize and sync new fragment
    - load new fragment
    - Atomic:
      - replace log fragment with new fragment
      - logger.committedBufferSz -= logFragment.size()
    - release log fragment
    - signal sleepers on logger.committedBufferSz

Worker: WriteConfigs

Worker: FileGc

Worker: CommitWait


#----------------------------------------------------------------------------
# Sequencing
#----------------------------------------------------------------------------

sequencer.nextTxn                 -- next txn to apply
sequencer.committedTxn            -- 


Commit stages:

  - Staged: txn allocated, queued for logger
  - Committed: cells logged

Sequencer:

  - nextTxn = 0
  - committedTxn = 0
  
  - futures = {}
  
  - atomic allocateTxn(): return nextTxn++
  - atomic commitTxn(): ++committedTxn; dispatch callbacks
  - atomic addCommitCallback(commitTargetTxn, callback)
  - atomic addFutureDeps(txn, nFutures):
    - txn (and anything after it) cannot be considered stable until
      another N future dependencies have been resolved and committed
 ** - this language is a bit strong.  maybe we can take out the (and
      anything after it) if we can guarantee that future txns will log
      the closure of their deps (possible?)
  - atomic resolveFutureDeps(txn, nFutures, futureTxn):
    - resolve N future deps on futureTxn


Cascade:  Table::open("foo")->set(X);

  - nextTxn == 1, committedTxn == 0
  - Queue commit: log cells (T=foo,P=X,txn=1,nextTxn=2)
  - Commit: log cells (T=foo,P=X)
    - create file log/1
    - create fragment F=log/1?t=foo
    - committedTxn = 1
    - add F to foo:1,foo:4,foo:5
      - Queue config (T=foo:1,txn=1)
        - add config dep: txn 1 depends on 1 config
      - Queue config (T=foo:4,txn=1)
        - add config dep: txn 1 depends on 2 configs 
      - Queue config (T=foo:5,txn=1)
        - add config dep: txn 1 depends on 3 configs 
  - Config: save (T=foo:1,txn=1)
    - Set data in META table
      - Queue commit: log cells (T=META,P=cf1,txn=2,nextTxn=3)
      - replace 1 config dep(s): txn 1 depends on txn 2 and 2 configs
  - Config: save (T=foo:4,txn=1), (T=foo:5,txn=1)  -- batch multiple
    - Set data in META table
      - Queue commit: log cells (T=META,P=cf4-5,txn=3,nextTxn=4)
      - replace 2 config dep(s): txn 1 depends on txn 3
        - (track max txn dep)
  - Commit: log cells (T=META,P=cf1)
    - reuse file log/1
    - create fragment F=log/1?t=META
    - committedTxn = 2
    - add F to META:0
      - Queue config: (T=META,txn=2)
        - add config dep: txn 2 depends on 1 config
  - Config: save (T=META:0,txn=2)
    - Queue commit: write root (T=META,P=cm0,txn=4,nextTxn=5)
      - replace 1 config dep(s): txn 2 depends on txn 4
  - Commit: log cells (T=META,P=cf4-5)
    - reuse file log/1
    - reuse fragment F=log/1?t=foo
    - committedTxn = 3
  - Commit: write root (T=META,P=cm0)
    - Write root config
    - committedTxn = 4
  

#----------------------------------------------------------------------------
# Writing
#----------------------------------------------------------------------------

When cells are applied, the block is assigned a txn number and pushed
to the logger thread's input queue.  At this point they are
"committed" but not "active".

The log thread wakes up and pulls blocks off the input queue, possibly
several at once if they're ready to go.  The blocks are partitioned
and merged to form a single block per table.  Each resulting block is
assigned the maximum txn number that went into it.  The blocks are
then serialized to a log file.  Once the log has been flushed to disk,
the blocks are added to the readable memory pool.  Each affected table
is notified of the new cell block.  The tables update transaction
tracking and queue config updates if necessary.  At this point, they
are "active" but not neccessarily "durable".  To be durable, any
config update transactions that resulted from the table notification
must also be durable.

Revision: How about this?

  - As soon as the incoming block is verified and assigned a commit
    txn, a fragment is built to wrap it, and it is added to the active
    set for the Table.  The block is also queued for logging.

  - Scanners can read active blocks immediately.  The data is not
    durable until it gets written to the log, though.  This shouldn't
    be a problem.  New RMW transactions can come in derived from the
    active-but-not-yet-durable data.  They will be serialized after,
    so if the earlier stuff doesn't make it, the later stuff won't
    make it either.

  - The log thread just pulls in commits, possibly aggregates, and
    writes blocks to the log file.  Every so often, it rolls to a new
    log file.  When a commit is sync'ed to disk, the transaction
    counter is notified, which may generate some callbacks for clients
    that were waiting on changes to become durable.  We track the
    transaction range covered by each log file.

  - Meanwhile, when adding these commit blocks to the active set, we
    may trigger a flushing compaction.  If the active data buffered in
    memory is too large, we want to compact at least some of it to
    disk fragments.

  - Log files can be reclaimed when the minimum transaction still held
    in memory for each table is greater than the maximum transaction
    held by the log file.


#----------------------------------------------------------------------------
# Scanning
#----------------------------------------------------------------------------

When scanning, we create a TableScanner over a table.  The table
scanner tracks the current tablet (row range) in the scan and the
chain of fragments used from that tablet.  It provides a merged view
of the fragment chain and tracks the necessary state to perform the
merge in increments.  Maintenance threads may replace or remove
fragments while the scan is in progress.  The scanner registers
interest in these kinds of events and receives asynchronous
notifications when they occur.

When a scanner is notified that fragments are being removed, it should
give up its references to them before returning from the notification
call.  The next incremental read call can rebuild the merge on the
new chain.

Scanners wishing to provide snapshot isolation on reads can attempt to
buffer cells needed from the current fragment chain.  The buffer
should be placed in a LRU cache shared with all other scanners.
Future reads may pull from the buffer, but the scanner should be able
to deal with the buffer getting evicted.

Scanners can also register for notification of fragments being added
to their current row range.  This will allow scanners that want to
read the absolute latest data to keep up to date.

(What scan transaction should the scanner return, and how does it get
that number?)

Tablets may split during a scan.  This isn't a problem for the
scanner, since it doesn't address tablets explicitly: it only cares
about its current row range and the events on it.

A tablet may be unloaded during a scan.  This should eventually cause
the scanner to throw a TabletNotLoadedError.


Table:

    Get the ordered chain of fragments to merge for the first part of
    the given predicate.  Returns true if such a chain is available
    (may be false if the tablet corresponding to the first part of the
    range is not loaded).  When the tablet is available, the fragment
    chain is returned in proper merge order.  The row interval over
    which the chain is valid is also returned.

    bool getFirstFragmentChain(
       ScanPredicate predicate,
       out vector<Fragment const*> fragmentChain,
       out Interval<string> rows) const;

Scanner:

   Table * table;
   FragMerge currentMerge;
   ScanPredicate pred;

   getNext()
   {
      if(
   }   

#----------------------------------------------------------------------------
# Fragments
#----------------------------------------------------------------------------

A fragment is an interface to a sorted, unique, immutable sequence of
cells within a single locality group/table.  Some fragments are backed
by a file on disk.  Some exist only in-memory (though they can be
reconstructed by replaying a log file).

Each fragment has an in-memory transaction number associated with it
representing the maximum possible transaction in the fragment.
Externally loaded fragments get the current transaction at load time.
Internally created fragments get the maximum transaction number from
the parts used to build it.


#----------------------------------------------------------------------------
# Configuration layout
#----------------------------------------------------------------------------

We need to store some information about each tablet:

  first-row: Lower bound for rows in the tablet

  last-row: Upper bound for rows in the tablet

  location: IP and port of current server (or last known server)

  log: Log directory of current server (or last known server)
  
  fragments:(group-name)
     Ordered list of serialized disk fragments in the given locality group


We also want some information for each table.

  schema:(group-name)
     Included column families (space-sep word list or *)
     Automatic deletion policy (max-history=K and/or max-age=T)
     Compression settings (none, lzo, etc)
     Disk block size (64k)
     In-memory setting (false/true)


Most tablets (all but one) will store their config information in a
parent meta-table.  The root meta tablet cannot store into a table so
it stores into a global file.

In current KDI, there is only one meta table (META).  It has only one
tablet that can never split, and it is the root.

As things grow, it may make sense to move to a two level approach.
We'd have two meta tables.  The first, META0, would have the single,
indivisible root tablet like the current implementation.  The second,
META1, would be a normal table that stores its config information in
META0.  Every other table would store its config information in META1.


The normal (non-root) tablets store their tablet configs their parent
meta table as follows:

  row="(table) (last-row)"    -- Tablets with a finite upper bound
  row="(table)!"              -- Tablets with infinite upper bound

  c="first:", t=(rev), v="(first-row)"
     -- should match previous tablet's last-row
     -- empty string for first tablet

  c="location:", t=(rev), v="(ip) (port)"

  c="log:", t=(rev), v="(log directory)"

  c="fragments:(group)", t=(rev), v="(f1) (f2) ..."
  
The per-table config information is stored in the same parent meta
table:

  row="(table)"

  c="schema:(group):columns", t=(rev), v="(c1) (c2) ..."

  c="schema:(group):purge", t=(rev), v="(deletion-policy)"
     -- policies:
       -- "max-history=(num-revisions)"
       -- "max-age=(microseconds)"
       -- "and" / "or" combination (e.g. "max-age=T and max-history=K")

  c="schema:(group):compression", t=(rev), v="(compressor-name)"

  c="schema:(group):blocksize", t=(rev), v="(size-in-bytes)"

  c="schema:(group):in-memory", t=(rev), v="true|false"


The root table(t) has to store the same information in one or more
global files.  However it all goes down, the "log:" and "fragments:"
columns should be updated in a synchronized way, which means they
should probably be in the same file.


#----------------------------------------------------------------------------
# More on Table Schemas
#----------------------------------------------------------------------------

Updating schemas...

An external process will update the "schema:(name)" columns for the
last tablet in the table.  Then we can either: 1) signal that the
tablet servers should reload the config, or 2) wait for the tablet
servers to discover the new columns in their own time.

When a tablet server discovers a new schema, it re-partitions all the
fragments it has for each tablet in the table into the new groups.
Fragments can be shared among multiple groups.  For example, suppose
we have the following schema change:

  Old schema:
     schema:   columns="*" ...
     schema:A  columns="X Y" ...
     schema:B  columns="Z" ...

  New schema:
     schema:   columns="*" ...
     schema:A  columns="X" ...
     schema:B  columns="Y" ...

If we have a fragment that was serialized under group A in the old
schema, it may be shared between groups A and B under the new schema.

Each disk fragment should contain a compact list of all column
families contained within.  The server will use this information in
assigning it to a group.

As compactions take place over time, the file structure on disk will
gradually transition to reflect the schema's partitioning structure.


Column family assignment...

Each group in the schema either as a set of column families or
wildcard meaning "every family not in another group".

There can be at most one wildcard group in a schema, but there can be
zero.  This restricts the column families in the table to a fixed set.
Tablet servers should reject mutations on columns not contained in the
set.

When adding columns to the schema, it may take time for the schema
changes to propagate to all tablet servers.  If clients need a known
cut-over point, they may need to use the active signaling method
mentioned above (1).

When removing columns from a schema, servers will stop serving the
columns and the old column data will eventually be filtered out by
compactions.

Updates to the schema columns should always be performed using row
transactions and read using row isolation semantics to ensure
consistency.


Schema validity...

Each column family can be in at most one group.  This implies there
can be at most one group containing a family wildcard.  If a column
could map to more than one group, the schema is invalid.

In the event that a server sees an invalid schema during a periodic
check, it should ignore the update and continue using the last valid
schema.  If the server finds an invalid schema when loading a tablet,
it should abort the load and return an error to the load RPC caller.
If the server sees an invalid schema during an explicit reload call,
it should return an error to the RPC caller and continue using the
last valid schema.


In-memory groups...

A group can be marked for residence in memory.  Each tablet server has
a fixed-size cache for fragment blocks.  When the server loads groups
with the in-memory attribute set, it will try to keep all of the
fragment blocks for that group in the cache at all times, allowing
scans to proceed without needing to access the disk.

In the event that the server is asked to load more fragment data than
it can hold in the cache, it will simply load as much as it can and
operate an an LRU fashion after that.

When schema changes move column families around, we can wind up with
fragments shared between multiple groups.  Cache priority is as
follows:

   1) All fragment columns in the same in-memory group
   2) All fragment columns in two or more in-memory groups
   3) Some fragment columns in at least one in-memory group
   4) No fragment columns in an in-memory group

Priorities 1-3 are all eligible for preloading.  Priority 4 will only
be loaded on-demand.

In general, the servers will do what they can to optimize performance,
but it may take time to rearrage data for efficient in-memory service
after a schema change.


#----------------------------------------------------------------------------
# Loading a tablet
#----------------------------------------------------------------------------

As soon as a tablet load starts, a placeholder tablet should be
created and registered in a loading state.  This tablet will not serve
client requests, but requests may be deferred until the tablet
completes loading (or fails to load).

When loading a tablet, the tablet server should read the tablet's
config information from its meta table.  It should also get a copy of
the table schema if it doesn't have one or the cached copy is old.

A tablet should only be served by one tablet server at a time.  Long
term, we'll probably want this to be arbitrated by an external
synchronization process.  As a sanity check, we could have the server
try to read the tablet from the host at "location:", if any.  If that
works, then some other server currently has an active copy of the
tablet.  Bad news!  Abort!

All fragments from all "fragments:" columns should be loaded up and
put in their appropriate access groups.

If the tablet has a "log:" column, then it wasn't unloaded cleanly.
Maybe its old server crashed.  We need to examine the log directory
and replay all logs that affect the tablet.  The logs must be replayed
in proper order if there is more than one.  It doesn't matter if we
replay more than we need to.

There are a few ways we could do the replay:

  1) Read the log(s), filter by tablet(s) to be loaded, and replay
     into our own log.  This reuses some of the normal mutation path,
     but it may involve special handling.  For example, if the log
     fills up, we'll need to serialize and update our tablets.  We
     don't want the loading tablet to perform any config updates until
     it's completely loaded and we can switch it over.

  2) Do what we're currently doing.  Load each log into memory, sort
     it, and write out a serialized fragment.  Use the new fragment
     instead.  This is simple, but it has potential scaling problems
     if we want to increase the log size.  Actually, I guess it's easy
     to write multiple fragments.  But it's still a rarely-used code
     path waiting to break.

  3) Perform a coordinated log rewrite after a server crash, like
     BigTable.  This would prevent potential overload of too many
     clients trying to read the log at once, but at our current scale
     we don't have this problem.  This is something to consider for
     the future.

After log replay, we can update the "log:" and "fragments:" columns
for the tablet to point to this server with the latest config.  Once
these changes sync, we can make the tablet writable and update the
"location:" column.  We can include the "location:" update in the
commit, so long as clients are willing to retry us if they read the
change and we haven't yet gotten the sync confirmation.


Further ruminations...

Table states:

  0) Table is unknown.

  1) Table allocated, schema is loading.

  2) Table is active.


Tablet states:

  0) Tablet is unknown.

     If we get a request for the Tablet, it's an error.

  1) Tablet allocated, config is loading.

     If we get a request for the Tablet, it is deferred for later.

  2) Tablet config loaded, replaying old log.

     At this point, we should have placeholder entries for all
     fragments.  New fragments may come in as the log is replayed.

     If we can support out-of-order commits, we can start logging
     incoming mutations while we replay the old logs.  This will allow
     us to clear buffer space to accept mutations on other Tablets.
     We can't acknowledge the commit yet, though.

     OOO commits might be feasible, but they're tricky.  It occurs to
     me that we're somewhat unlikely to get writes on this tablet at
     this stage.  It could happen if the server crashed and bounced,
     serving at the same address and port as the old server instance.
     It could also happen if some client just made a lucky guess.

     On the other hand, writes for other active Tablets can come while
     we're replaying the log.  We need to make sure that they can
     proceed without problems.

  3) Tablet log loaded, config is saving.

     The new config will indicate that the Tablet is now hosted by the
     loading server and depends on the server's log.  The old log
     reference is overwritten.  The config will include all old disk
     fragments plus any new fragments that have come in during log
     replay.

  4) Tablet config synced, fragments are loading.

     We can safely enable writes and acknowledge syncs for this tablet
     now.  Mutations will be applied to this server's log.  If this
     server crashes, future servers will replay the logs.

     Read requests are still deferred.

  5) Tablet is active.

     With all of the fragments loaded, we can allow normal read and
     write traffic.

  6) Tablet is read-only.  In-memory data is being compacted.

     The Tablet is in the process of being unloaded.  We can't take
     any more writes, but it is still okay to service read requests.
     The in-memory data for the Tablet is being compacted to disk.

  7) Tablet is disabled with all data on disk.  Config is saving.

     We're writing the Tablet's final config.  The config indicates
     that it is no longer hosted by this server or references this
     server's log directory.  We've disconnected all readers for the
     Tablet.  We can drop all in-memory state for the Tablet except
     for the fact that the Tablet is in this state.  Once the config
     syncs, we can move back to 0.


(TabletState > 0 ) ==> (TableState > 0)

OOO commits:

   We could name log files to correspond with the first transaction in
   them.

   When loading a log directory, we can reserve transaction numbers
   for the entire log set.  It seems like we should only need 1.

   Say we choose transaction T for the log reload.  We can make sure
   that the log rolls such that everything less than T is separate
   from everything greater than T.  Then we can replay the old log
   into its own file at T.

   That fixes the log files, but it doesn't help the in-memory or
   compaction problems.

   It would be much nicer to simply read the blocks out of the old log
   files and pass them to the normal apply commit path.

   Instead of applying new commits interleaved with log replay, we
   could queue the requests in a side channel that could spill to disk
   if it got too full.  After the tablet finishes loading, we can
   replay the queue as well.

   Or we can just block.





#----------------------------------------------------------------------------
# Unloading a tablet
#----------------------------------------------------------------------------

Stop accepting writes for the tablet.

Serialize anything we have for the tablet to a new fragment.

Stop accepting reads.

Update config with new "fragments:" and remove "log:" and "location:" columns.

Drop tablet.


#----------------------------------------------------------------------------
# Packed Formats
#----------------------------------------------------------------------------

Line format:

  - Untrusted: should be able to accept any random data without crashing
  - Fast and compact
  - Ability to build in-place

Log format:

  - More trusted than line format.  Rely on checksum for corruption
    detection but don't worry about malicious bits.

Disk format:

  - Efficient storage
  - Pluggable compression
  - Fast row seek


#----------------------------------------------------------------------------
# Serialization : flushing mem-fragments to disk to free space for more
#----------------------------------------------------------------------------

Each Table has a list of cached Fragments that are available for
reading but not yet backed by a disk fragment.  Some of these
Fragments will be recorded in the server's log file, while some have
yet to make it into the log.

Serialize task:

  - Find the "best" Table to flush.  Best can mean the one hogging the
    most memory, or the one covering the most on-disk log space.  A
    combination of the two is probably best:
      (A * mem-size + B * log-size)

  - Merge all mem fragments available at the beginning of the merge to
    a new disk fragment.  Sync the fragment to disk.

    - The merge should only cover currently loaded/active Tablet
      ranges.  Tablets that have already been unloaded or are in the
      process of unloading have their data pulled out separately.

    - During the merge, it may make sense to build an intersection of
      the output rows with currently loaded tablet ranges.  Or just
      make a list of all output rows.  This will help in deciding
      which Tablets need to add the fragment.

  - Coordinate these steps in some way that makes sense:
  
    - Add the new disk fragment to all Tablets that overlap with the
      output.  Initiate config writes.  When the config writes sync,
      we release references to the log files (but not before).

    - Remove the fragments from the Table's mem fragment list.

    - Notify everyone referencing the serialized fragments that they
      need to drop said references.  Not sure how to do this.

Questions:

  - What happens when tablets are loading and unloading in the mean
    time?

  - How do we indicate that log files aren't needed any more?
    Similarly, how do we decide how much log space a Table is using?



#----------------------------------------------------------------------------
# Shared fragments and schema changes
#----------------------------------------------------------------------------

    Trying to unify fragment lists by topological sorting has issues:

    Schema = X:(a b) Y:(c d)
    {
        X: Y:
        Insert 1=[a c]
        X:1 Y:1
        Insert 2=[a b]
        X:1,2 Y:1
        Insert 3=[c d]
        X:1,2 Y;1,3
    }
    Schema = X:(a b c) Y:(d)
    {
        X:1,2,3 Y:3
        Compact X, 4=[a b c]
        X:4 Y:3
    }
    Schema - X:(a) Y:(b c d)
    {
        X:4 Y:4,3  <-- broken: 3[c] comes after 4[c]
    }


    (Broken) Solution: when each fragment is created or loaded, assign
    a sequence number.  The sequence number server-local -- not
    written to disk or communicated anywhere.  When saving fragments,
    they must always be saved in increasing sequence order.  Broken:
    suppose we compact 1+2 above to 4.  The correct order in X would
    be 4,3.

    Fix(?) for above: we track two numbers: creation sequence and load
    sequence.  Primary ordering is by load sequence, and creation
    sequence is a tie-breaker.  When compacting, the load sequence of
    the output is the max load sequence of the input.  So above we'd
    have:
        Insert 1-1
        Insert 2-2
        Insert 3-3
        Compact (1-1,2-2,3-3) -> 3-4

    


    How about if erasures get filtered out, as in a rooted compaction?

    Schema = X:(a b) Y:(c d)
    {
        X: Y:
        Insert 1=[a c]
        X:1 Y:1
        Insert 2=[a b]
        X:1,2 Y:1
        Insert 3=[c d]
        X:1,2 Y;1,3
        Insert 4=[-c]  (erases all 1[c] and 3[c])
        X:1,2,4 Y;1,3
    }
    Schema = X:(a b c) Y:(d)
    {
        X:1,2,3,4 Y:3
        Compact X, 5=[a b]
        X:5 Y:3
    }
    Schema = X:(a) Y:(b c d)
    {
        X:5 Y:3,5  <-- broken: 3[c] is back
    }


    Solution(?): only allow erasure filtering on columns that have no
    fragments containing them in other groups.  That is, every
    fragment containing the column is in column's current group and no
    other.  This might be correct, but it can make it very hard to
    reclaim erasures after a schema change.  Boo...  Otoh, maybe this
    is just the price of doing a column-shifting schema change.  They
    shouldn't be done often.  Also, the condition to allow filtering
    is a pain to track.


    In both cases above, the problem could be avoided by restricting
    fragment 3 to [d].

    Solution(?): save column restrictions for fragments in tablet
    configs.  Default is no restrictions.



  The Plan:

    When loading:
    
      colChain = {}  # col -> [fn1, fn2, ...]
      fragCols = {}  # fn -> set(col1, col2, ..)

      for fn, cols in config.fragments:
         for col in cols.split():
            colChain.setdefault(col,[]).append(fn)
            fragCols.setdefault(fn,set()).add(col)
      
      for g in groups:
         chains = [ colChain[c] for c in group.columns if c in colChain ]
         g.frags = [ loadFrag(fn, fragCols[fn].intersection(group.columns))
                     for fn in topoMerge(chains) ]


    When saving:

      out = []
      for g in groups:
         for f in g.frags:
            out.append( (f.getFilename(), f.getColumns()) )
      config.fragments = out


    On schema change, drop all fragments and reload them.

      This can proceed one Tablet at a time.  Probably the fastest
      thing would be to swap all the old fragment pointers into a
      temporary vector, clear the tablet vectors, reload all, and drop
      the temp vector.  The point of the temporary is to keep cached
      fragments from getting destroyed.  Leaky abstractions are the
      best!


#----------------------------------------------------------------------------
# Group / Tablet layout
#----------------------------------------------------------------------------

Disk fragments are partitioned between locality groups and tablets.
The partitioning is not necessarily disjoint and can change over time.

Operations:

  - Serialize: A serialization completes and needs to add the
    serialized Fragment to each affected Tablet.  Usually the Fragment
    will map to one Group, but that may not be the case if the schema
    just changes.  (Is that safe?)

  - Compact: A compaction completes and needs to replace a list of
    Fragments with a new Fragment.  Same concern as above.

  - Scan: Get a merged view of all Fragments in a Tablet that match a
    ScanPredicate.


  - Load: loading a Tablet; we must add multiple Fragments

  - Unload: unloading a Tablet

  - Schema change

  - Add commit to mem pool

  - Pick a mem-group to serialize:
       argmax (A * mem-size(x) + B * log-size(x))
       mem-size(x) = sum ( frag-size(f) for f in x.frags )
       log-size(x) = sum ( file-size(f) for f in allLogFiles
                           if max-commit(f) >= min-commit(x) )

Layout:

  Table {
  }

  TabletServer {
 

  }

#----------------------------------------------------------------------------
# Config writing
#----------------------------------------------------------------------------

When do tablet configs need to be saved?

  1. After loading, we need to save the config so that the Tablet has
     the new server's log directory and location.

  2. After unloading, we need to save with the final fragment list and
     remove the server's log directory and location.

  3. Split/join?  Not sure about this one.  It seems like it's not
     essential for split or joined tablets to have their configs saved
     immediately, but the update ought to be saved in one operation.
     We'll need a way to delete old configs as well as save new ones.

     "Same time" saving/deleting is probably too strict.  In a meta
     table, the configs will be in different rows, potentially on
     different servers.  Let's not get into that kind of transaction
     mess.  Figure out a way to store configs so we can recover
     gracefully.

  4. We should "eventually" save the config after serialization and
     compaction, so that old disk files can be cleaned up by the GC.

     A. For serialization, we want to clean up dead files in the log
        directory so we don't have to replay them later in the event
        of a server crash.  We also want to reclaim the disk space.

        When can the log GC reclaim a file?

        Log GC is local to the server controlling the log directory.
        
        Each log file covers a compact sequence of commits.  Each log
        file can be shared between multiple Tables.  A log file is no
        longer needed when all Tables have serialized past the last
        commit in the log file AND the resulting fragments from the
        serializion(s) have been recorded for all affected Tablets'
        configs.


     B. For compaction, we want to reclaim disk space and avoid
        recompacting on another server in the event of a server crash.

        When can the disk fragment GC reclaim a file?

        Disk fragment GC is global.  Fragment files exist in a global
        namespace and may be shared between Tablets on different
        Servers.

        The GC can reclaim a file that is not referenced (or soon to
        be referenced) by any Tablet.  A file is "referenced" by a
        Tablet if it is named in the Tablet's config.  It is "soon to
        be referenced" if a server could add it to a Tablet's config
        at some point in the future.

        The "referenced" condition is easy enough for the GC to check
        by reading Tablet configs.  The "soon to be referenced"
        condition is more difficult.

        STBR 1: The GC asks each server for its oldest STBR file
        creation time.  The GC then ignores any file younger than the
        oldest STBR file for all servers.  Use the start-time of the
        GC round as a maximum time (always ignore files created after
        the GC started up).  Assumption: files in the same namespace
        share a consistent creation clock.  Annoyance: have to talk to
        all servers to do GC.

        STBR 2: The GC looks at all candidate files and chooses a
        cutoff time that is some amount of time (1 day?) older than the
        most recent.  It ignores all files newer than that.  Problems:
        how much garbage can we accumulate in the cutoff window?  What
        if some server is going really slow?  If nothing is changing,
        we'll never clean the last.

        STBR 3: STBR files are explicitly named by servers that own
        them.  Problems: overhead, race on new files.

        STBR 4: Like STBR 1, except servers report their oldest STBR
        ctime when doing periodic check-ins with a master server.  The
        GC just asks the master for the oldest ctime among the
        currently active servers.
